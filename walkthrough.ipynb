{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ariel ML Challenge Baseline\n",
    "\n",
    "Notebook presenting the baseline model for the [Ariel ML challenge 2021](https://www.ariel-datachallenge.space/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "    \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to data dirs\n",
    "lc_train_path = \"/Users/mario/data/ariel_ml_2021/home 3/ucapats/Scratch/ml_data_challenge/training_set/noisy_train\"\n",
    "params_train_path = \"/Users/mario/data/ariel_ml_2021/home/ucapats/Scratch/ml_data_challenge/training_set/params_train\"\n",
    "lc_test_path = \"/Users/mario/data/ariel_ml_2021/home 2/ucapats/Scratch/ml_data_challenge/test_set/noisy_test\"\n",
    "\n",
    "# Import Dataset class \n",
    "from utils import ArielMLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's plot a random spectral light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = ArielMLDataset(lc_train_path, params_train_path, shuffle=True)\n",
    "\n",
    "idx = np.random.randint(len(dataset))\n",
    "item = dataset[idx]\n",
    "offsets = np.linspace(-0.05, 0.05, item['lc'].shape[0])\n",
    "f, ax = plt.subplots(figsize=(13,9))\n",
    "plt.plot(item['lc'].T.detach().numpy() + offsets , label=None)\n",
    "ax.legend([round(x, 4) for x in item['target'].detach().numpy()], fontsize=6, loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define simples preprocessing steps\n",
    "- smoothing \n",
    "- clipping\n",
    "- normalisation per wavelength\n",
    "- removing ramp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simple_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include these steps in the datasets for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 32\n",
    "val_size = 32\n",
    "test_size = 1024\n",
    "\n",
    "# Training\n",
    "dataset_train = ArielMLDataset(lc_train_path, params_train_path, shuffle=True, start_ind=0, \n",
    "                               max_size=train_size, transform=simple_transform)\n",
    "# Validation\n",
    "dataset_val = ArielMLDataset(lc_train_path, params_train_path, shuffle=True, start_ind=train_size, \n",
    "                             max_size=val_size, transform=simple_transform)\n",
    "\n",
    "# Testing\n",
    "dataset_test = ArielMLDataset(lc_train_path, params_train_path, start_ind=train_size+val_size, \n",
    "                              shuffle=True, max_size=test_size, transform=simple_transform)\n",
    "\n",
    "# Evaluation : no output path available here, this will only be used for submission\n",
    "dataset_eval = ArielMLDataset(lc_test_path, shuffle=True, transform=simple_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the corresponding data loaders, still using Pytorch utils module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = int(train_size / 4)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size)\n",
    "loader_eval = DataLoader(dataset_eval, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Metric\n",
    "\n",
    "The scoring system used for evaluation is defined here: https://www.ariel-datachallenge.space/ML/documentation/scoring\n",
    "\n",
    "Let's define it here, with unity weights as we don't have the actual weights available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ChallengeMetric\n",
    "    \n",
    "challenge_metric = ChallengeMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constant prediction model for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_1 = lambda x: torch.ones(x.shape[:-1]) * 0.06  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model, a fully connected neural network with 2 hidden layers with ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Baseline\n",
    "    \n",
    "baseline = Baseline().double().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "\n",
    "opt = Adam(baseline.parameters())\n",
    "loss_function = MSELoss()  # Alternatives could be ChallengeMetric() or L1Loss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_scores = []\n",
    "best_val_score = 0.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_from = 10\n",
    "epochs = 60\n",
    "\n",
    "\n",
    "for epoch in range(1, 1+epochs):\n",
    "    print(epoch)\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    for k, item in enumerate(loader_train):\n",
    "        pred = baseline(item['lc'])\n",
    "        loss = loss_function(item['target'], pred)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()    \n",
    "        train_loss += loss.detach().item()\n",
    "    train_loss = train_loss / len(loader_train)\n",
    "    for k, item in enumerate(loader_val):\n",
    "        pred = baseline(item['lc'])\n",
    "        loss = loss_function(item['target'], pred)\n",
    "        score = challenge_metric.score(item['target'], pred)\n",
    "        val_loss += loss.detach().item()\n",
    "        val_score += score.detach().item()\n",
    "    val_loss /= len(loader_val)\n",
    "    val_score /= len(loader_val)\n",
    "    print('Training loss', round(train_loss, 6))\n",
    "    print('Val loss', round(val_loss, 6))\n",
    "    print('Val score', round(val_score, 2))\n",
    "    train_losses += [train_loss]\n",
    "    val_losses += [val_loss]\n",
    "    val_scores += [val_score]\n",
    "    \n",
    "    if epoch >= save_from and val_score > best_val_score:\n",
    "        torch.save(baseline, 'outputs/model_state.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, '-o', label='Train Loss')\n",
    "plt.plot(val_losses, '-o', label='Val Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel(loss_function)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.plot(val_scores, '-o', label='Val Score')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Challenge score (unity weights)')\n",
    "# plt.yscale('log')\n",
    "plt.ylim(5000,10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(loader_test))\n",
    "\n",
    "preds = {'naive1': naive_1(item['lc']), \n",
    "         'normal_1000ppm': torch.normal(item['target'], 1e-3),\n",
    "         'baseline': baseline(item['lc'])\n",
    "        }\n",
    "\n",
    "for name, pred in preds.items():\n",
    "    print(name, f\"\\t{challenge_metric(item['target'], pred).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce evaluation vectors\n",
    "(takes a few mins to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = torch.load('outputs/model_state.pt')\n",
    "baseline.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tqdm\n",
    "preds = []\n",
    "\n",
    "for k, item in tqdm.tqdm(enumerate(loader_eval)):\n",
    "    preds += [baseline(item['lc'])]\n",
    "\n",
    "eval_pred = torch.cat(preds).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly plot the mean results per wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eval_pred.mean(0), '-o')\n",
    "plt.xlabel('wavelength')\n",
    "plt.ylabel('mean prediction per wavelength')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally save the results as a txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'outputs/baseline_evaluation.txt'\n",
    "if save_path and (53900, 55) == eval_pred.shape:\n",
    "    np.savetxt(save_path, eval_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
