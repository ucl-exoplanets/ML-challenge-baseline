{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk through the baseline model for the [Ariel ML challenge 2021](https://www.ariel-datachallenge.space/).\n",
    "\n",
    "What this notebooks does: \n",
    "- access the data using an ArielMLDataset utility class\n",
    "- plot a random input light curve\n",
    "- use some simple preprocessing steps for the input light curves\n",
    "- train the baseline model for a subset of the data (by default)\n",
    "- produce an evaluation file for the baseline model\n",
    "\n",
    "What this notebooks does *not*:\n",
    "- train on the full dataset\n",
    "- explore alternative training losses\n",
    "- optimise of the current model architecture\n",
    "- discuss other models or architectures\n",
    "- include additional parameters (only the light curves are used) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "    \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic \n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to data dirs\n",
    "lc_train_path = \"data/noisy_train/ucapats/Scratch/ml_data_challenge/training_set/noisy_train\"\n",
    "params_train_path = \"data/params_train/home/ucapats/Scratch/ml_data_challenge/training_set/params_train\"\n",
    "lc_test_path = \"data/noisy_test/home/ucapats/Scratch/ml_data_challenge/test_set/noisy_test\"\n",
    "\n",
    "# Import Dataset class \n",
    "from utils import ArielMLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's plot a random spectral light curve, just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = ArielMLDataset(lc_train_path, params_train_path, shuffle=True)\n",
    "\n",
    "idx = np.random.randint(len(dataset))\n",
    "item = dataset[idx]\n",
    "offsets = np.linspace(-0.05, 0.05, item['lc'].shape[0])\n",
    "f, ax = plt.subplots(figsize=(13,9))\n",
    "plt.plot(item['lc'].T.detach().numpy() + offsets , label=None)\n",
    "ax.legend([round(x, 4) for x in item['target'].detach().numpy()], fontsize=6, loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define simples preprocessing steps\n",
    "- smoothing \n",
    "- clipping\n",
    "- normalisation per wavelength\n",
    "- removing ramp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simple_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include these steps in the datasets for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "train_size = 512\n",
    "val_size = 1024\n",
    "test_size = 1024\n",
    "\n",
    "# Training\n",
    "dataset_train = ArielMLDataset(lc_train_path, params_train_path, shuffle=True, start_ind=0, \n",
    "                               max_size=train_size, transform=simple_transform, seed=random_seed)\n",
    "# Validation\n",
    "dataset_val = ArielMLDataset(lc_train_path, params_train_path, shuffle=True, start_ind=train_size, \n",
    "                             max_size=val_size, transform=simple_transform, seed=random_seed)\n",
    "\n",
    "# Testing\n",
    "dataset_test = ArielMLDataset(lc_train_path, params_train_path, start_ind=train_size+val_size, \n",
    "                              shuffle=True, max_size=test_size, transform=simple_transform, seed=random_seed)\n",
    "\n",
    "# Evaluation : no output path available here, this will only be used for submission\n",
    "dataset_eval = ArielMLDataset(lc_test_path, shuffle=False, transform=simple_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the corresponding data loaders, still using Pytorch utils module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = int(train_size / 4)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size)\n",
    "loader_eval = DataLoader(dataset_eval, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge's Metric\n",
    "\n",
    "The scoring system used for evaluation is defined here: https://www.ariel-datachallenge.space/ML/documentation/scoring\n",
    "\n",
    "Let's define it roughly here, with unity weights as we don't have the actual weights available. Note that this might likely lead to conservative (pessimistic) score estimation as the real metric gives smaller weights to the hardest samples (in terms of signal-to-noise), whereas here all the samples have equal weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ChallengeMetric\n",
    "    \n",
    "challenge_metric = ChallengeMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constant prediction model for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_1 = lambda x: torch.ones(x.shape[:-1]) * 0.06  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model, a fully connected neural network with 2 hidden layers with ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Baseline\n",
    "    \n",
    "baseline = Baseline().double().to(device)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "\n",
    "opt = Adam(baseline.parameters(), lr=0.0005)\n",
    "loss_function = MSELoss()  # Alternatives could be ChallengeMetric() or L1Loss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_scores = []\n",
    "best_val_score = 0.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_from = 3\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(1, 1+epochs):\n",
    "    print(epoch)\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    baseline.train()\n",
    "    for k, item in enumerate(loader_train):\n",
    "        pred = baseline(item['lc'])\n",
    "        loss = loss_function(item['target'], pred)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()    \n",
    "        train_loss += loss.detach().item()\n",
    "    train_loss = train_loss / len(loader_train)\n",
    "    \n",
    "    baseline.eval()\n",
    "    for k, item in enumerate(loader_val):\n",
    "        pred = baseline(item['lc'])\n",
    "        loss = loss_function(item['target'], pred)\n",
    "        score = challenge_metric.score(item['target'], pred)\n",
    "        val_loss += loss.detach().item()\n",
    "        val_score += score.detach().item()\n",
    "    val_loss /= len(loader_val)\n",
    "    val_score /= len(loader_val)\n",
    "    print('Training loss', round(train_loss, 6))\n",
    "    print('Val loss', round(val_loss, 6))\n",
    "    print('Val score', round(val_score, 2))\n",
    "    train_losses += [train_loss]\n",
    "    val_losses += [val_loss]\n",
    "    val_scores += [val_score]\n",
    "    \n",
    "    if epoch >= save_from and val_score > best_val_score:\n",
    "        best_val_score = val_score\n",
    "        torch.save(baseline, 'outputs/model_state.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "plt.plot(train_losses, '-o', label='Train Loss', markersize=2)\n",
    "plt.plot(val_losses, '-o', label='Val Loss', markersize=2)\n",
    "plt.xlabel('epochs', fontsize=12)\n",
    "plt.ylabel('Mean-Squared Error', fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "secay = ax.twinx()\n",
    "secay.plot()\n",
    "secay.plot(val_scores, '-o', label='Val Score', markersize=2, color='black')\n",
    "secay.set_xlabel('epochs', fontsize=12)\n",
    "secay.set_ylabel('Challenge score (unity weights)', fontsize=12)\n",
    "secay.set_ylim(5000,10000)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's reload the model with the best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = torch.load('outputs/model_state.pt')\n",
    "baseline.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(loader_test))\n",
    "\n",
    "preds = {'naive1': naive_1(item['lc']), \n",
    "         'normal_1000ppm': torch.normal(item['target'], 1e-3),\n",
    "         'baseline': baseline(item['lc'])\n",
    "        }\n",
    "\n",
    "for name, pred in preds.items():\n",
    "    print(name, f\"\\t{challenge_metric.score(item['target'], pred).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce evaluation vectors\n",
    "(takes a several mins to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tqdm\n",
    "preds = []\n",
    "\n",
    "for k, item in tqdm.tqdm(enumerate(loader_eval)):\n",
    "    preds += [baseline(item['lc'])]\n",
    "\n",
    "eval_pred = torch.cat(preds).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly plot the mean results per wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eval_pred.mean(0), '-o')\n",
    "plt.xlabel('wavelength')\n",
    "plt.ylabel('mean prediction per wavelength')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally save the results as a txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "save_path = f'outputs/baseline_evaluation_{datetime.datetime.today().date()}.txt'\n",
    "if save_path and (53900, 55) == eval_pred.shape:\n",
    "    np.savetxt(save_path, eval_pred, fmt='%.10f', delimiter='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pylightcurve-torch)",
   "language": "python",
   "name": "pylightcurve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
